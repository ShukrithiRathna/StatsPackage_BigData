{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining\n",
    "\n",
    "The cleaned dataset (Modified_plants.csv) was mined using various techniques to obtaining interesting rules and patters  by implementing  the following algorithms:\n",
    "  * FIM:\n",
    "    * Apriori\n",
    "    * FP Growth\n",
    "  * CFI\n",
    "    * A-Close (modified version of apriori)\n",
    "  * MFI\n",
    "    * FP max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from apyori import apriori\n",
    "from mlxtend.frequent_patterns import apriori as a_close\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import fpmax\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm\n",
    "* To implement thhe alogirthm,  the cleaned dataset was converted into a  'list of lists' format. \n",
    "* The  'Symbol' column is split into it's consitutent letters to get meanignful insights.\n",
    "* The resultant rules obtained are written to a file in the csv format. The top five rows are displayed below. \n",
    "\n",
    "### Evaluation Measures:\n",
    "* The measures of Confidence and Lift were used to evaluate the rules mined. \n",
    "* Confidence\n",
    "    * Conf(X=>Y) = Supp(X\\cupY) \\div Supp(X) –\n",
    "    * It measures how often each item in Y appears in transactions that contains items in X also.\n",
    "* Lift\n",
    "    * LiftX=>Y) = Conf(X=>Y) \\div Supp(Y)\n",
    "    * Lift value near 1 indicates X and Y almost often appear together as expected.\n",
    "    * Greater than 1 means they appear together more than expected.\n",
    "    * Less than 1 means they appear less than expected.\n",
    "    * Greater lift values indicate stronger association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Symbol Syn_Symbol       Family    Sci_Name_1  Sci_Name_2\n0  ANDRO5      JUEC2  Acanthaceae  Andrographis       Wall.\n1    ANEC      JUEC2  Acanthaceae  Andrographis   echioides\n2    ANEC      JUEC2  Acanthaceae      Justicia   echioides\n3   DIBR2      JUEC2  Acanthaceae    Dicliptera   brachiata\n4   DIBR2      DIBR6  Acanthaceae     Diapedium  brachiatum\n                       Rule   Support  Confidence      Lift\n0     Aster -- > Asteraceae  0.013829    1.000000  8.112179\n1              Aster -- > S  0.011458    0.828571  3.502975\n2              Aster -- > Y  0.010404    0.752381  8.285466\n3  Solidago -- > Asteraceae  0.010734    1.000000  8.112179\n4              Carex -- > C  0.027328    1.000000  3.295573\n"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('Datasets/Modified_Plants.csv')\n",
    "print(data.head(5))\n",
    "df = []\n",
    "\n",
    "for _,row in data.iterrows():\n",
    "    value = []\n",
    "    for item in row['Symbol']:\n",
    "        value.append(item)\n",
    "    for item in row[1:]:\n",
    "        value.append(item)\n",
    "    df.append(value) \n",
    "\n",
    "rules = apriori(df, min_support=0.01, min_confidence=0.4, min_lift=3, min_length=2)\n",
    "rules = list(rules)\n",
    "apriori_results = pd.DataFrame(columns=['Rule','Support','Confidence','Lift'])\n",
    "# print(apriori_results.head(5))\n",
    "for rule in rules:\n",
    "    supp = rule[1]\n",
    "    conf = rule[2][0][-2]\n",
    "    lift = rule[2][0][-1]\n",
    "\n",
    "    Rule = ''\n",
    "    Rule += next(iter(rule[2][0][0]))\n",
    "    for item in rule[2][0][1:-3]:\n",
    "        Rule += ',' + next(iter(item))\n",
    "    Rule += ' -- > '\n",
    "    Rule += next(iter(rule[2][0][-3]))\n",
    "    \n",
    "    row = [Rule,supp,conf,lift]\n",
    "\n",
    "    apriori_results = apriori_results.append(pd.Series(row,index=apriori_results.columns),ignore_index=True)\n",
    "print(apriori_results.head(5))\n",
    "apriori_results.to_csv('Rules/Apriori_rules.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP Growth Algorithm\n",
    "* The given dataset contains a large number of uniwue values in the 'Family' column and as such is not suitable for encoding. \n",
    "* Therefore the dataset is grouped by 'Family' and only those families with number of rows >=500 are chosen for encoding.\n",
    "* After grouping by family, the data is grouped by symbol, and only those with more than 25 rows are taken. \n",
    "* The resultant datset contains 345 rows and 6 columns. - It is written into a csv file - 'Pruned_Data.csv'\n",
    "* All further analysis is done on this dataset.  \n",
    "* The data is enoded using the transaction encoder which is a part of the mlxtend library. \n",
    "* The resultant rules obtained are written to a file in the csv format. The top five rows are displayed below.\n",
    "\n",
    "Evaluation Measures:\n",
    "The following measures were used to evaluate the rules mined:\n",
    "* Confidence\n",
    "    * Conf(X=>Y) = Supp(X\\cupY) \\div Supp(X) \n",
    "    * It measures how often each item in Y appears in transactions that contains items in X also.\n",
    "\n",
    "* Lift\n",
    "    * LiftX=>Y) = Conf(X=>Y) \\div Supp(Y)\n",
    "    * Lift value near 1 indicates X and Y almost often appear together as expected.\n",
    "    * Greater than 1 means they appear together more than expected.\n",
    "    * Less than 1 means they appear less than expected.\n",
    "    * Greater lift values indicate stronger association.\n",
    "\n",
    "* Conviction \n",
    "    * It determines which part of the association rule has the upper hand; whether the L.H.S drives the R.H.S or vice versa.\n",
    "    * Conviction(A →B) = (1- support(B)) / (1- Confidence(A →B)).\n",
    "    * A high conviction value means that the consequent is highly depending on the antecedent. \n",
    "    * In the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as inf . \n",
    "    * Similar to lift, if items are independent, the conviction is 1.\n",
    "\n",
    "* A rule can be divided into two parts - antecedent and consequent. \n",
    "    * The antecedent refers to the condition, clause or item, and the consequent refers to what it implies or its consequence. \n",
    "    * The suppoer for the antecedent as well as the consequent is also calculated.\n",
    "\n",
    "* Leverage:\n",
    "    * levarage(A→C)=support(A→C)−support(A)×support(C),range: [−1,1]\n",
    "    * Leverage computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent. \n",
    "    * A leverage value of 0 indicates independence.\n",
    "\n",
    "* Conviction:\n",
    "    * conviction(A→C)=1−support(C)1−confidence(A→C),range: [0,∞]\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rule  Antecedent Support  Consequent Support   Support  \\\n0    {l,S} --- > {o}            0.005797            0.005797  0.005797   \n1    {o} --- > {l,S}            0.005797            0.005797  0.005797   \n2    {y,S} --- > {o}            0.005797            0.005797  0.005797   \n3    {o} --- > {y,S}            0.005797            0.005797  0.005797   \n4  {l,y,S} --- > {o}            0.005797            0.005797  0.005797   \n\n   Confidence   Lift  Conviction  \n0         1.0  172.5         inf  \n1         1.0  172.5         inf  \n2         1.0  172.5         inf  \n3         1.0  172.5         inf  \n4         1.0  172.5         inf  \n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_rule(x,y):\n",
    "    rule = '{'\n",
    "    rule += list(x)[0]\n",
    "    for item in list(x)[1:]:\n",
    "        rule += ',' + item\n",
    "    rule += '} --- > {' + list(y)[0]\n",
    "    for item in list(y)[1:]:\n",
    "        rule += ',' + item\n",
    "    rule += '}'\n",
    "    return rule\n",
    "\n",
    "\n",
    "# data = pd.read_csv('Modified_Plants.csv')\n",
    "\n",
    "data = data[data.groupby('Family')['Family'].transform('size') > 100]\n",
    "data = data[data.groupby('Symbol')['Symbol'].transform('size') > 25]\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# print(data.shape)\n",
    "# print(data.head(5))\n",
    "# data.to_csv('Pruned_Data.csv',index=False)\n",
    "#print(data.Family.value_counts())\n",
    "\n",
    "te = TransactionEncoder()\n",
    "enc_basket = te.fit(data).transform(data)\n",
    "\n",
    "# print(enc_basket)\n",
    "\n",
    "basket=pd.DataFrame(enc_basket, columns=te.columns_)\n",
    "basket.to_csv('Encoded_data.csv', index=False)\n",
    "\n",
    "fp_items=fpgrowth(basket, min_support=0.005, use_colnames=True)\n",
    "\n",
    "fp_rules = association_rules(fp_items, metric =\"lift\", min_threshold = 1) \n",
    "fp_rules = fp_rules.sort_values(['confidence', 'lift'], ascending =[False, False]) \n",
    "fp_rules.drop([\"leverage\"], axis = 1, inplace = True)\n",
    "\n",
    "fp_rules.reset_index(drop=True,inplace=True)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "fp_rules['Rule'] = fp_rules.apply(lambda row : get_rule(row['antecedents'],row['consequents']),axis=1)\n",
    "fp_rules = fp_rules[['Rule','antecedent support','consequent support','support','confidence','lift','conviction']]\n",
    "fp_rules.columns = ['Rule','Antecedent Support','Consequent Support','Support','Confidence','Lift','Conviction']\n",
    "\n",
    "print(fp_rules.head(5))\n",
    "fp_rules.to_csv('Rules/FP_Rules.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximal Frequent Itemset\n",
    "* An itemset is maximal frequent if none of its supersets are frequent.\n",
    "\n",
    "#### Closed Frequent Itemset\n",
    "* An itemset is closed if none of its immediate supersets have same support count same as Itemset.\n",
    "\n",
    "\n",
    "### A-Close Algorithm (CFI) and FP Max (MFI)\n",
    " * The encoded dataset obtained in the previous stage was used to implement the the a-close and the FP Max algorithms.\n",
    " * The a-close is a modified version of apriori, to mine Closed frequent itemsets. \n",
    " * The FP Max algorithm is a  modified version of FP growth algorithm to mine Maximal Frequent itemsets.\n",
    " * Similar to apriori and FP growth the results obtained were evaluated using various measures such as confidence, lift, conviction and leverage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "antecedents consequents  antecedent support  consequent support   support  \\\n0         (m)         (S)            0.014493            0.011594  0.011594   \n1         (S)         (m)            0.011594            0.014493  0.011594   \n\n   confidence  lift  leverage  conviction  \n0         0.8  69.0  0.011426    4.942029  \n1         1.0  69.0  0.011426         inf  \n"
    }
   ],
   "source": [
    "# A-Close\n",
    "basket = pd.read_csv('Encoded_data.csv')\n",
    "a_close_itemsets= a_close(basket, min_support=0.01, use_colnames=True)\n",
    "a_rules = association_rules(a_close_itemsets, metric =\"lift\", min_threshold = 1) \n",
    "print(a_rules.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "support                     itemsets\n0  0.002899  (i, 1, m, e, c, N, S, _, a)\n1  0.002899  (i, m, e, c, N, 2, S, _, a)\n2  0.002899           (i, m, y, l, F, a)\n3  0.002899     (m, o, y, n, S, _, l, b)\n"
    }
   ],
   "source": [
    "# fpmax\n",
    "\n",
    "fp_max_itemsets = fpmax(basket, min_support=0.0005, use_colnames=True,max_len = 10)\n",
    "print(fp_max_itemsets)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondad012a44a650f45058c40ce762f760b38",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}